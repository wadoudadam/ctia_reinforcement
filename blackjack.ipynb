{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning with Gymnasium ( Blackjack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fujitsu\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "# Let's start by creating the blackjack environment.\n",
    "# Note: We are going to follow the rules from Sutton & Barto.\n",
    "# Other versions of the game can be found below for you to experiment.\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\", sab=True,render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the environment to get the first observation\n",
    "done = False\n",
    "observation, info = env.reset()\n",
    "\n",
    "\n",
    "## Visualisation de l'environnement\n",
    "        frame = env.render()\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "#observation = (16,9,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a random action from all valid actions\n",
    "action = env.action_space.sample()\n",
    "# action=1\n",
    "\n",
    "# execute the action in our environment and receive infos from the environment\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# observation=(24, 10, False)\n",
    "# reward=-1.0\n",
    "# terminated=True\n",
    "# truncated=False\n",
    "# info={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "discount_factor = 0.95\n",
    "learning_rate = 0.01\n",
    "n_episodes = 100\n",
    "epsilon = 1.0\n",
    "epsilon_decay = epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "training_error = []\n",
    "observation, info = env.reset()\n",
    "# Défini l'action suivante\n",
    "\n",
    "def get_action(obs):\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(q_values[obs]))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm SARSA or SARSA(0) se base sur l'équation ci-après:\n",
    "\n",
    "\n",
    "    target = reward + (gamma * Q[next_state][next_action])               # construct TD target\n",
    "    new_value = Q[state][action] + (alpha * (target - current))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mise à jour de la valeur de q par l'algorithm du td learning\n",
    "\n",
    "def update(\n",
    "        obs,\n",
    "        action,\n",
    "        reward,\n",
    "        terminated,\n",
    "        next_obs,\n",
    "    ):\n",
    "    \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "    future_q_value = (not terminated) * (q_values[next_obs][action])\n",
    "    temporal_difference = (\n",
    "            reward + discount_factor * future_q_value - q_values[obs][action]\n",
    "        )\n",
    "\n",
    "    q_values[obs][action] = (\n",
    "            q_values[obs][action] + learning_rate * temporal_difference\n",
    "        )\n",
    "    return q_values[obs][action]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "n_episodes = 100\n",
    "\n",
    "final_epsilon = 0.1\n",
    "\n",
    "## Mise à jour de l'episode\n",
    "def decay_epsilon(epsilon,episode):\n",
    "    epsilon_decay = epsilon / (episode / 2)  # reduce the exploration over time\n",
    "    epsilon = max(final_epsilon,epsilon - epsilon_decay)\n",
    "    return epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Attempted to add episode stats when they already exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-39f3f51f8ce7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# choix de la nouvelle action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# update de la q-value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Fujitsu\\anaconda3\\lib\\site-packages\\gymnasium\\wrappers\\record_episode_statistics.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum_dones\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m\"episode\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfos\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"_episode\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minfos\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m     96\u001b[0m                     \u001b[1;34m\"Attempted to add episode stats when they already exist\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: Attempted to add episode stats when they already exist"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    clear_output()\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        # choix de la nouvelle action\n",
    "        action = get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update de la q-value\n",
    "        q_values[obs][action] = update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        ## Visualisation de l'environnement\n",
    "        frame = env.render()\n",
    "        plt.imshow(frame)\n",
    "        plt.show()\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "        decay_epsilon(1.0, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
